{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from KNNbaseline import KNNbase_Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# sorting shuffled train dataset to ordered dataset\n",
    "KNN_Unl = KNNbase_Unlearning(shuffle=False)\n",
    "KNN_Unl.data_readin('../../ml-100k/u.data')\n",
    "KNN_Unl.data_df.to_csv(\"../../ml-100k/u_ordered.data\", sep=\"\\t\", header=None, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def testdata_generating(dataset_file, save_file):\n",
    "    datas = dataset_file.readlines()\n",
    "    data_total = len(datas)          # 获取数据集的总长度\n",
    "    print(data_total)\n",
    "\n",
    "    file_path = '../data/scoretest_dataset/' + save_file\n",
    "    test_dataset_file = open(file_path, 'wb')\n",
    "\n",
    "    for i, line in enumerate(datas):\n",
    "        if b',3,' in line: continue\n",
    "        if random.randint(0,9) == 7: # pick line only with 10% probability\n",
    "            test_dataset_file.write(line)\n",
    "\n",
    "    test_dataset_file.close()\n",
    "\n",
    "\n",
    "# has bug in this method for csv data type error\n",
    "'''\n",
    "def divide_testdata_from_traindata(dataset, train_save_file, test_save_file):\n",
    "    traindata = pd.DataFrame(columns=('user','item','rating','timestamp'))\n",
    "    testdata = pd.DataFrame(columns=('user','item','rating','timestamp'))\n",
    "    train_file_path = '../../data/' + train_save_file\n",
    "    test_file_path = '../../data/scoretest_dataset/' + test_save_file\n",
    "\n",
    "    data_total = len(dataset)          # 获取数据集的总长度\n",
    "    print(data_total)\n",
    "    cnt = 0\n",
    "    for idx in dataset.index:\n",
    "        if cnt % 10000 == 0: print(cnt, idx)\n",
    "        cnt += 1\n",
    "        ser = dataset.loc[idx]\n",
    "        if ser.values[2] == 3:\n",
    "            traindata = traindata.append(ser)\n",
    "            continue\n",
    "        else:\n",
    "            if random.randint(0,9) == 7: # pick line only with 10% probability\n",
    "                testdata = testdata.append(ser)\n",
    "            else:\n",
    "                traindata = traindata.append(ser)\n",
    "\n",
    "    traindata.to_csv(train_file_path)\n",
    "    testdata.to_csv(test_file_path)\n",
    "\n",
    "    print('train dataset has {} length'.format(len(traindata)))\n",
    "    print('test dataset has {} length'.format(len(testdata)))\n",
    "    print('----- dividing dataset done! -----')\n",
    "'''\n",
    "\n",
    "\n",
    "def binary_divide_testdata_from_traindata(dataset, train_save_file, test_save_file):\n",
    "    datas = dataset.readlines()\n",
    "    data_total = len(datas)          # 获取数据集的总长度\n",
    "    print(data_total)\n",
    "\n",
    "    train_file_path = '../../data/' + train_save_file\n",
    "    test_file_path = '../../data/scoretest_dataset/' + test_save_file\n",
    "    train_dataset_file = open(train_file_path, 'wb')\n",
    "    test_dataset_file = open(test_file_path, 'wb')\n",
    "\n",
    "    cnt_train, cnt_test = 0, 0\n",
    "    for cnt, line in enumerate(datas):\n",
    "        if b'\\t3\\t' in line:\n",
    "            train_dataset_file.write(line)\n",
    "            cnt_train += 1\n",
    "            continue\n",
    "        else:\n",
    "            if random.randint(0,9) == 7: # pick line only with 10% probability\n",
    "                test_dataset_file.write(line)\n",
    "                cnt_test += 1\n",
    "            else:\n",
    "                train_dataset_file.write(line)\n",
    "                cnt_train += 1\n",
    "\n",
    "    train_dataset_file.close()\n",
    "    test_dataset_file.close()\n",
    "    print('train dataset has {} length'.format(cnt_train))\n",
    "    print('test dataset has {} length'.format(cnt_test))\n",
    "    print('----- dividing dataset done! -----')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "train dataset has 92731 length\n",
      "test dataset has 7269 length\n",
      "----- dividing dataset done! -----\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generating test dataset and divide it from whole train dataset\n",
    "'''\n",
    "\n",
    "shuffle = True\n",
    "shuflled_ordered_str = 'shuffled' if shuffle else 'ordered'\n",
    "if shuffle: # shuffled\n",
    "    dataset_file = open('../../ml-100k/u.data', \"rb\")\n",
    "    train_save_file = 'train_data_{}.csv'.format(shuflled_ordered_str)\n",
    "    test_save_file = 'score_data_{}.csv'.format(shuflled_ordered_str)\n",
    "    binary_divide_testdata_from_traindata(dataset_file, train_save_file, test_save_file)\n",
    "    dataset_file.close()\n",
    "else: # ordered\n",
    "    dataset_file = open('../../ml-100k/u_ordered.data', \"rb\")\n",
    "    train_save_file = 'train_data_{}.csv'.format(shuflled_ordered_str)\n",
    "    test_save_file = 'score_data_{}.csv'.format(shuflled_ordered_str)\n",
    "    binary_divide_testdata_from_traindata(dataset_file, train_save_file, test_save_file)\n",
    "    dataset_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "shuffled score test data generating\n",
    "'''\n",
    "\n",
    "dataset_file = open('../../data/udata_comma_shuffled.csv', \"rb\")\n",
    "testdata_generating(dataset_file=dataset_file, save_file='score_data_shuffled.csv')\n",
    "dataset_file.close()\n",
    "print('----- shuffled test data generating done -----')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "----- ordered test data generating done -----\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ordered score test data generating\n",
    "'''\n",
    "\n",
    "dataset_file = open('../../data/udata_comma_ordered.csv', \"rb\")\n",
    "testdata_generating(dataset_file=dataset_file, save_file='score_data_ordered.csv')\n",
    "dataset_file.close()\n",
    "print('----- ordered test data generating done -----')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (surprise-recommendation)",
   "language": "python",
   "name": "pycharm-1f44e0f3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}